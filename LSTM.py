# ä¸‹è½½è¯­æ–™

from tensorflow import keras
import numpy as np

path = keras.utils.get_file(
    'nietzsche.txt', 
    origin='')
text = open(path).read().lower()
print('Corpus length:', len(text))
# å°†å­—ç¬¦åºåˆ—å‘é‡åŒ–

maxlen = 60     # æ¯ä¸ªåºåˆ—çš„é•¿åº¦
step = 3        # æ¯ 3 ä¸ªå­—ç¬¦é‡‡æ ·ä¸€ä¸ªæ–°åºåˆ—
sentences = []  # ä¿å­˜æ‰€æå–çš„åºåˆ—
next_chars = [] # sentences çš„ä¸‹ä¸€ä¸ªå­—ç¬¦

for i in range(0, len(text) - maxlen, step):
    sentences.append(text[i: i+maxlen])
    next_chars.append(text[i+maxlen])
print('Number of sequences:', len(sentences))

chars = sorted(list(set(text)))
char_indices = dict((char, chars.index(char)) for char in chars)
# æ’ï¼šä¸Šé¢è¿™ä¸¤è¡Œä»£ç  6
print('Unique characters:', len(chars))

print('Vectorization...')

x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)
y = np.zeros((len(sentences), len(chars)), dtype=np.bool)

for i, sentence in enumerate(sentences):
    for t, char in enumerate(sentence):
        x[i, t, char_indices[char]] = 1
    y[i, char_indices[next_chars[i]]] = 1
# ç”¨äºé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦çš„å•å±‚ LSTM æ¨¡å‹

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense

model = Sequential()
model.add(LSTM(128, input_shape=(maxlen, len(chars))))
model.add(Dense(len(chars), activation='softmax'))
# æ¨¡å‹ç¼–è¯‘é…ç½®

from tensorflow.keras import optimizers

optimizer = optimizers.RMSprop(lr=0.01)
model.compile(loss='categorical_crossentropy',
              optimizer=optimizer)
def sample(preds, temperature=1.0):
    '''
    å¯¹æ¨¡å‹å¾—åˆ°çš„åŸå§‹æ¦‚ç‡åˆ†å¸ƒé‡æ–°åŠ æƒï¼Œå¹¶ä»ä¸­æŠ½å–ä¸€ä¸ªå­—ç¬¦ç´¢å¼•
    '''
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)
# æ–‡æœ¬ç”Ÿæˆå¾ªç¯

import random

for epoch in range(1, 60):    # è®­ç»ƒ 60 ä¸ªè½®æ¬¡
    print(f'ğŸ‘‰\033[1;35m epoch {epoch} \033[0m')    # print('epoch', epoch)
    
    model.fit(x, y,
              batch_size=128,
              epochs=1)
    
    start_index = random.randint(0, len(text) - maxlen - 1)
    generated_text = text[start_index: start_index + maxlen]
    print(f'  ğŸ“– Generating with seed: "\033[1;32;43m{generated_text}\033[0m"')    # print(f' Generating with seed: "{generated_text}"')
    
    for temperature in [0.2, 0.5, 1.0, 1.2]:
        print(f'\n   \033[1;36m ğŸŒ¡ï¸ temperature: {temperature}\033[0m')    # print('\n  temperature:', temperature)
        print(generated_text, end='')
        for i in range(400):    # ç”Ÿæˆ 400 ä¸ªå­—ç¬¦
            # one-hot ç¼–ç ç›®å‰æœ‰çš„æ–‡æœ¬
            sampled = np.zeros((1, maxlen, len(chars)))
            for t, char in enumerate(generated_text):
                sampled[0, t, char_indices[char]] = 1
            
            # é¢„æµ‹ï¼Œé‡‡æ ·ï¼Œç”Ÿæˆä¸‹ä¸€å­—ç¬¦
            preds = model.predict(sampled, verbose=0)[0]
            next_index = sample(preds, temperature)
            next_char = chars[next_index]
            print(next_char, end='')
            
            generated_text = generated_text[1:] + next_char
            
    print('\n' + '-' * 20)
import random
import tensorflow as tf
from tensorflow.keras import optimizers
from tensorflow.keras import layers
from tensorflow.keras import models
from tensorflow import keras
import numpy as np

import jieba    # ä½¿ç”¨ jieba åšä¸­æ–‡åˆ†è¯
import os

os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

# å¯¼å…¥æ–‡æœ¬

path = '~/Desktop/txt_zh_cn.txt'
text = open(path).read().lower()
print('Corpus length:', len(text))

# å°†æ–‡æœ¬åºåˆ—å‘é‡åŒ–

maxlen = 60     # æ¯ä¸ªåºåˆ—çš„é•¿åº¦
step = 3        # æ¯ 3 ä¸ª token é‡‡æ ·ä¸€ä¸ªæ–°åºåˆ—
sentences = []  # ä¿å­˜æ‰€æå–çš„åºåˆ—
next_tokens = []  # sentences çš„ä¸‹ä¸€ä¸ª token

token_text = list(jieba.cut(text))

tokens = list(set(token_text))
tokens_indices = {token: tokens.index(token) for token in tokens}
print('Number of tokens:', len(tokens))

for i in range(0, len(token_text) - maxlen, step):
    sentences.append(
        list(map(lambda t: tokens_indices[t], token_text[i: i+maxlen])))
    next_tokens.append(tokens_indices[token_text[i+maxlen]])
print('Number of sequences:', len(sentences))

# å°†ç›®æ ‡ one-hot ç¼–ç 
next_tokens_one_hot = []
for i in next_tokens:
    y = np.zeros((len(tokens),), dtype=np.bool)
    y[i] = 1
    next_tokens_one_hot.append(y)

# åšæˆæ•°æ®é›†
dataset = tf.data.Dataset.from_tensor_slices((sentences, next_tokens_one_hot))
dataset = dataset.shuffle(buffer_size=4096)
dataset = dataset.batch(128)
dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)


# æ„å»ºã€ç¼–è¯‘æ¨¡å‹

model = models.Sequential([
    layers.Embedding(len(tokens), 256),
    layers.LSTM(256),
    layers.Dense(len(tokens), activation='softmax')
])

optimizer = optimizers.RMSprop(lr=0.1)
model.compile(loss='categorical_crossentropy',
              optimizer=optimizer)

# é‡‡æ ·å‡½æ•°

def sample(preds, temperature=1.0):
    '''
    å¯¹æ¨¡å‹å¾—åˆ°çš„åŸå§‹æ¦‚ç‡åˆ†å¸ƒé‡æ–°åŠ æƒï¼Œå¹¶ä»ä¸­æŠ½å–ä¸€ä¸ª token ç´¢å¼•
    '''
    preds = np.asarray(preds).astype('float64')
    preds = np.log(preds) / temperature
    exp_preds = np.exp(preds)
    preds = exp_preds / np.sum(exp_preds)
    probas = np.random.multinomial(1, preds, 1)
    return np.argmax(probas)

# è®­ç»ƒæ¨¡å‹

callbacks_list = [
    keras.callbacks.ModelCheckpoint(  # åœ¨æ¯è½®å®Œæˆåä¿å­˜æƒé‡
        filepath='text_gen.h5',
        monitor='loss',
        save_best_only=True,
    ),
    keras.callbacks.ReduceLROnPlateau(  # ä¸å†æ”¹å–„æ—¶é™ä½å­¦ä¹ ç‡
        monitor='loss',
        factor=0.5,
        patience=1,
    ),
    keras.callbacks.EarlyStopping(  # ä¸å†æ”¹å–„æ—¶ä¸­æ–­è®­ç»ƒ
        monitor='loss',
        patience=3,
    ),
]

model.fit(dataset, epochs=30, callbacks=callbacks_list)

# æ–‡æœ¬ç”Ÿæˆ

start_index = random.randint(0, len(text) - maxlen - 1)
generated_text = text[start_index: start_index + maxlen]
print(f' ğŸ“– Generating with seed: "{generated_text}"')

for temperature in [0.2, 0.5, 1.0, 1.2]:
    print('\n  ğŸŒ¡ï¸ temperature:', temperature)
    print(generated_text, end='')
    for i in range(100):    # ç”Ÿæˆ 100 ä¸ª token
        # ç¼–ç å½“å‰æ–‡æœ¬
        text_cut = jieba.cut(generated_text)
        sampled = []
        for i in text_cut:
            if i in tokens_indices:
                sampled.append(tokens_indices[i])
            else:
                sampled.append(0)

        # é¢„æµ‹ï¼Œé‡‡æ ·ï¼Œç”Ÿæˆä¸‹ä¸€ä¸ª token
        preds = model.predict(sampled, verbose=0)[0]
        next_index = sample(preds, temperature)
        next_token = tokens[next_index]
        print(next_token, end='')

        generated_text = generated_text[1:] + next_token

